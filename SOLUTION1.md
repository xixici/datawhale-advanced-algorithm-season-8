# 【Task1 随机森林算法梳理】
【参考框架】欢迎有自己的框架

## 1. 集成学习概念

组合多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。

## 2. 个体学习器概念

是指通过现有的机器学习算法进行学习训练。

## 3. boosting  bagging

boosting：其中包含的个体学习是同质的个体学习器，个体学习器之间存在强依赖关系，其中包含的个体学习器需要串行生成。
Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

bagging：与boosting类似，其包含的个体学习器也是同质的，但是个体学习器之间不存在强的依赖关系，其中的个体也是并行生成。
bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。

## 4. 结合策略(平均法，投票法，学习法)

- 平均法：对于若干个弱学习器的输出进行平均得到最终的预测输出。用于数值回归问题。

- 投票法：假设我们的预测类别是{c1,c2,...cK},对于任意一个预测样本x，我们的T个弱学习器的预测结果分别是(h1(x),h2(x)...hT(x))。T个弱学习器的对样本x的预测结果中，数量最多的类别ci为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。
用于分类预测。

- 学习法：将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。

## 5. 随机森林思想

随机森林是一种集成算法（Ensemble Learning），它属于Bagging类型，通过组合多个弱分类器，最终结果通过投票或取均值，使得整体模型的结果具有较高的精确度和泛化性能。其可以取得不错成绩，主要归功于“随机”和“森林”，一个使它具有抗过拟合能力，一个使它更加精准。

## 6. 随机森林的推广

RF是弱分类器，具有随机性，对于降低模型的方差很有作用，有较好的泛化能力和抗过拟合能力

## 7. 优缺点

- 由于采用了集成算法，本身精度比大多数单个算法要好
- 在测试集上表现良好，由于两个随机性的引入，使得随机森林不容易陷入过拟合（样本随机，特征随机）
- 在工业上，由于两个随机性的引入，使得随机森林具有一定的抗噪声能力，对比其他算法具有一定优势
- 由于树的组合，使得随机森林可以处理非线性数据，本身属于非线性分类（拟合）模型
它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化 训练速度快，可以运用在大规模数据集上

- 可以处理缺省值（单独作为一类），不用额外处理
- 由于有袋外数据（OOB），可以在模型生成过程中取得真实误差的无偏估计，且不损失训练数据量
-  在训练过程中，能够检测到feature间的互相影响，且可以得出feature的重要性，具有一定参考意义
- 由于每棵树可以独立、同时生成，容易做成并行化方法
- 由于实现简单、精度高、抗过拟合能力强，当面对非线性数据时，适于作为基准模型

## 8. sklearn参数

```
class sklearn.ensemble.RandomForestClassifier(n_estimators=10, crite-rion=’gini’,max_depth=None,min_samples_split=2, min_samples_leaf=1,
min_weight_fraction_leaf=0.0,max_features=’auto’,max_leaf_nodes=None, bootstrap=True,
oob_score=False, n_jobs=1, ran-dom_state=None, verbose=0,warm_start=False, class_weight=None)


```
### 决策树的参数：

- criterion: ”gini” or “entropy”(default=”gini”)是计算属性的gini(基尼不纯度)还是entropy(信息增益)，来选择最合适的节点。

- splitter: ”best” or “random”(default=”best”)随机选择属性还是选择不纯度最大的属性，建议用默认。
- max_features: 选择最适属性时划分的特征不能超过此值。当为整数时，即最大特征数；当为小数时，训练集特征数*小数；
```
if “auto”, then max_features=sqrt(n_features).
If “sqrt”, thenmax_features=sqrt(n_features).
If “log2”, thenmax_features=log2(n_features).
If None, then max_features=n_features.
```
- max_depth: (default=None)设置树的最大深度，默认为None，这样建树时，会使每一个叶节点只有一个类别，或是达到min_samples_split。
- min_samples_split:根据属性划分节点时，每个划分最少的样本数。
- min_samples_leaf:叶子节点最少的样本数。
- max_leaf_nodes: (default=None)叶子树的最大样本数。
- min_weight_fraction_leaf: (default=0) 叶子节点所需要的最小权值
- verbose:(default=0) 是否显示任务进程
### 随机森林特有的参数：
- n_estimators=10：决策树的个数，越多越好，但是性能就会越差，至少100左右（具体数字忘记从哪里来的了）可以达到可接受的性能和误差率。 
  bootstrap=True：是否有放回的采样。  
  oob_score=False：oob（out of band，带外）数据，即：在某次决策树训练中没有被bootstrap选中的数据。多单个模型的参数训练，我们知道可以用cross validation（cv）来进行，但是特别消耗时间，而且对于随机森林这种情况也没有大的必要，所以就用这个数据对决策树模型进行验证，算是一个简单的交叉验证。性能消耗小，但是效果不错。  

- n_jobs=1：并行job个数。这个在ensemble算法中非常重要，尤其是bagging（而非boosting，因为boosting的每次迭代之间有影响，所以很难进行并行化），因为可以并行从而提高性能。1=不并行；n：n个并行；-1：CPU有多少core，就启动多少job

- warm_start=False：热启动，决定是否使用上次调用该类的结果然后增加新的。  

- class_weight=None：各个label的权重。


## 9.应用场景

数据维度相对低（几十维），同时对准确性有较高要求时。
因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。